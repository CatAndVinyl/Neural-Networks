{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq6H1eK0bfYG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "f = open(\"/content/input.txt\", \"r\")\n",
        "raw_text = f.read()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "\n",
        "c_to_i = {j:i for i,j in zip(range(len(chars)), chars)}\n",
        "i_to_c = {i:j for i,j in zip(range(len(chars)), chars)}\n",
        "encode = lambda s : [c_to_i[c] for c in s]\n",
        "decode = lambda l : ''.join([i_to_c[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(raw_text)).to(device)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "val = data[n:]"
      ],
      "metadata": {
        "id": "GNAbuO152bhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 32\n",
        "n_embeddings = 384\n",
        "n_heads = 6\n",
        "dropout = 0.2\n",
        "vocab_size = len(chars)\n",
        "\n",
        "def get_batch(split=\"train\"):\n",
        "  if(split == \"train\"):\n",
        "    batch_data = train\n",
        "  else:\n",
        "    batch_data = val\n",
        "  rand_vals = torch.randint(len(batch_data) - block_size, (batch_size,))\n",
        "  x = torch.stack([batch_data[i:i+block_size] for i in rand_vals])\n",
        "  y = torch.stack([batch_data[i+1:i+block_size+1] for i in rand_vals])\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "EkkxBiVYDHx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention_Head(torch.nn.Module):\n",
        "  def __init__(self, n_embeddings, head_size):\n",
        "    super().__init__()\n",
        "    self.Q_weights = torch.nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    self.K_weights = torch.nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    self.V_weights = torch.nn.Linear(n_embeddings, head_size, bias=False)\n",
        "    self.sqrt_head_size = math.sqrt(head_size)\n",
        "    self.reg = torch.nn.Dropout(dropout)\n",
        "    #self.attention_layer = torch.nn.Linear(head_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    #attention head\n",
        "    Q = self.Q_weights(idx) # (B, T, head_size)\n",
        "    K = self.K_weights(idx) # (B, T, head_size)\n",
        "    V = self.V_weights(idx) # (B, T, head_size)\n",
        "\n",
        "    W = Q @ K.transpose(-1,-2) # (B, T, T)\n",
        "\n",
        "    W = torch.tril(W) / self.sqrt_head_size + torch.triu(torch.full_like(W, float(\"-inf\")), 1)\n",
        "    W = F.softmax(W, dim = -2) #(B, T, T)\n",
        "    W = self.reg(W)\n",
        "\n",
        "    Att = W @ V #(B, T, head_size)\n",
        "\n",
        "    return Att"
      ],
      "metadata": {
        "id": "o1ic_YFT8SZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class multi_attention_head(torch.nn.Module):\n",
        "  def __init__(self, n_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = torch.nn.ModuleList([Attention_Head(n_embeddings, head_size) for i in range(n_heads)])\n",
        "    self.proj = torch.nn.Linear(n_embeddings,n_embeddings)\n",
        "    self.reg = torch.nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    mha_out = torch.concat([h(idx) for h in self.heads], dim =-1)\n",
        "    projections = self.proj(mha_out)\n",
        "    out = self.reg(projections)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DC1BuDAPAJ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ffwd(torch.nn.Module):\n",
        "  def __init__(self, n_embeddings):\n",
        "    super().__init__();\n",
        "    self.linear = torch.nn.Linear(n_embeddings, 4 * n_embeddings)\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    self.proj = torch.nn.Linear(4 * n_embeddings, n_embeddings)\n",
        "    self.reg = torch.nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    raw_activations = self.linear(idx)\n",
        "    relu_act = self.activation(raw_activations)\n",
        "    projections = self.proj(relu_act)\n",
        "    out = self.reg(projections)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DlC2nIU_nfZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(torch.nn.Module):\n",
        "  def __init__(self, n_embeddings, n_headds):\n",
        "    super().__init__();\n",
        "    self.ma_head = multi_attention_head(n_heads, n_embeddings//n_heads)\n",
        "    self.feed_forward = ffwd(n_embeddings)\n",
        "    self.ln1 = torch.nn.LayerNorm(n_embeddings)\n",
        "    self.ln2 = torch.nn.LayerNorm(n_embeddings)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    add_norm_mha = idx + self.ma_head(self.ln1(idx)) #(B, T, C) + #(B, T, C)\n",
        "    add_norm_ffwd = add_norm_mha + self.feed_forward(self.ln2(add_norm_mha))\n",
        "    return add_norm_ffwd"
      ],
      "metadata": {
        "id": "lP03yeBsMZml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding_table = torch.nn.Embedding(vocab_size,n_embeddings)\n",
        "    self.pos_embeddings = torch.nn.Embedding(block_size, n_embeddings)\n",
        "    self.blocks = torch.nn.Sequential(\n",
        "        Block(n_embeddings, n_heads),\n",
        "        Block(n_embeddings, n_heads),\n",
        "        Block(n_embeddings, n_heads)\n",
        "    )\n",
        "    self.lm_head = torch.nn.Linear(n_embeddings, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    #C = num_embeddings\n",
        "    tok_emb = self.embedding_table(idx) #(B, T, C)\n",
        "    pos_emb = self.pos_embeddings(torch.arange(T, device=device)) #(T, C)\n",
        "    x = tok_emb + pos_emb # (B, T, C)\n",
        "\n",
        "    raw_output = self.blocks(x)\n",
        "\n",
        "    logits = self.lm_head(raw_output) #(B, T, vocab_size)\n",
        "    #print(tok_emb.shape, pos_emb.shape)\n",
        "    #logits = self.lm_head(W) #(B, T, vocab_size)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      logits, loss = self(idx[:,-20:])\n",
        "      pred = F.softmax(logits[:,-1,:], dim=-1)\n",
        "      nv = torch.multinomial(pred, 1)\n",
        "      idx = torch.cat((idx, nv), dim = 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "ic5jEq4NEhas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramModel()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s18dumtWse_",
        "outputId": "219d1604-f2a8-4741-8927-e0ecbb2d960f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramModel(\n",
              "  (embedding_table): Embedding(65, 384)\n",
              "  (pos_embeddings): Embedding(20, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (ma_head): multi_attention_head(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Attention_Head(\n",
              "            (Q_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (K_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (V_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (reg): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): ffwd(\n",
              "        (linear): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (activation): ReLU()\n",
              "        (proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (ma_head): multi_attention_head(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Attention_Head(\n",
              "            (Q_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (K_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (V_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (reg): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): ffwd(\n",
              "        (linear): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (activation): ReLU()\n",
              "        (proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (ma_head): multi_attention_head(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Attention_Head(\n",
              "            (Q_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (K_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (V_weights): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (reg): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): ffwd(\n",
              "        (linear): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (activation): ReLU()\n",
              "        (proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (reg): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_tensor = torch.tensor([[0]]).to(device)\n",
        "zed = model.generate(init_tensor, 1000)"
      ],
      "metadata": {
        "id": "gaDV13ThaKAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([decode(i) for i in zed.tolist()][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGM-jGIVpIuo",
        "outputId": "3b087e13-383e-45a5-ceee-bece0cc5390f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OFo oe o\n",
            "O a n Aeat: whe toos of thom has rear senctered, coshat king igherare he 's ede hife the is lot save timendw the\n",
            "wich'd Lo down ad, prakin iar's po sortour, nd gotlied yould achmomay swand to string\n",
            "Apbacid ound sdtithes sus gord, angleble baight thonbe andand,\n",
            "Whath sinve of stengwailed, he'r nowranve\n",
            "BaWo his of his eeqet Low thaly'd trive is de moke my and my as, bre dis weard awite bo sepe lovetow\n",
            "Good le soughou deag hois ford'ceits y to pasterem gank,\n",
            "With pekes sigh be ince my lovee.\n",
            "\n",
            "CORIAR got by swawhion the ave he.\n",
            "\n",
            "TENVERMLANG! Fowthe\n",
            "Wo tman is thatand the is kill Eves, whou toon be tho beys\n",
            "righad harimait, and your egarwancenp\n",
            "Thoor kigh ofind hathis 'th driee\n",
            "BOKNGHEO:\n",
            "Whmaued.\n",
            "\n",
            "Fith miill ifsabd faimny sat tiy him thal\n",
            "They dis\n",
            "\n",
            "inier forme how havn wold hat once!\n",
            "\n",
            "LOKNG Lidtof;\n",
            "And Eo dhent no detoone: Low! Wist your sair ang-peit-lal;\n",
            "Rome, gut-on a I' As waced\n",
            "lo:\n",
            "Ad is a prosterj chou our is,\n",
            "socts; qukes, thond what 'terenldeviir kinder.\n",
            "\n",
            "Hot frore, ny lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "val_interval = 300\n",
        "\n",
        "for steps in range(5000):\n",
        "  optimizer.zero_grad()\n",
        "  batch_x, batch_y = get_batch()\n",
        "  lgts, loss = model(batch_x, batch_y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if(steps % val_interval == 0):\n",
        "    val_x, val_y = get_batch(\"val\")\n",
        "    val_loss = model(val_x, val_y)[1]\n",
        "    print(loss, val_loss)"
      ],
      "metadata": {
        "id": "_1Zvhs1i3g0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "d4c67008-20c2-4cff-c601-541542d515bf"
      },
      "execution_count": null,
      "outputs": [
        
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQS6eo6I3UY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
